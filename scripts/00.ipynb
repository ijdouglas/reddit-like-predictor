{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3HytLQt-3G1","outputId":"07427191-3556-4d2a-bf93-5e8d2cdeb4f5","executionInfo":{"status":"ok","timestamp":1669139954685,"user_tz":360,"elapsed":2985,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Projects/reddit-vote-predictor')\n","import sys\n","if 'psaw' not in sys.modules:\n","  print('Installing psaw')\n","  os.system('pip install psaw')\n","from datetime import datetime\n","from scripts.RedditScrape import * # imports a class `RedditScrape`"],"metadata":{"id":"jKF-p4HH_Cbo","executionInfo":{"status":"ok","timestamp":1669139961028,"user_tz":360,"elapsed":3438,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"93cf6b78-134f-4247-a3e2-0d280f0f27a8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing psaw\n"]}]},{"cell_type":"code","source":["# Set date range from which to extract data for two subreddits\n","# NOTE: ater running the data scraping and cleaning pipeline, one year of data\n","# resulted in around 1000 images, so we'll add time to the date range\n","# 10 years from the start of 2011 to the end of 2020\n","start_ = datetime(2011, 1, 1)\n","end_ = datetime(2020, 12, 31)\n","# Run the scraping pipeline using custom RedditScrape class imported above\n","cat = RedditScrape('cat', start_, end_)\n","cat.scrape_posts()\n","cat.posts.to_csv('data/tbl/cat_posts.csv', index=False)\n","dog = RedditScrape('dog', start_, end_)\n","dog.scrape_posts()\n","dog.posts.to_csv('data/tbl/dog_posts.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1JiHU41_VP_","outputId":"6ef691e7-168e-46d0-c4fd-85e028e81d54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n","  warnings.warn(shards_down_message)\n","/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n","  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n","/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n","  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"]}]}]}