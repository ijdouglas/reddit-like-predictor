{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19183,"status":"ok","timestamp":1669771380202,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"},"user_tz":360},"id":"e3HytLQt-3G1","outputId":"072d641e-4f5d-41d3-f9a2-571954a46549"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"jKF-p4HH_Cbo","outputId":"486d4efe-e9d4-453f-d3d0-72913fec63db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Installing psaw\n"]}],"source":["import os\n","os.chdir('/content/drive/MyDrive/Projects/reddit-vote-predictor')\n","import sys\n","if 'psaw' not in sys.modules:\n","  print('Installing psaw')\n","  os.system('pip install psaw')\n","from datetime import datetime\n","from scripts.RedditScrape import * # imports a class `RedditScrape`\n","# Reddit's own api:\n","import praw"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":817807,"status":"ok","timestamp":1669140795600,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"},"user_tz":360},"id":"p1JiHU41_VP_","outputId":"6ef691e7-168e-46d0-c4fd-85e028e81d54"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n","  warnings.warn(shards_down_message)\n","/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n","  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n","/usr/local/lib/python3.7/dist-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n","  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"]}],"source":["# Set date range from which to extract data for two subreddits\n","# NOTE: ater running the data scraping and cleaning pipeline, one year of data\n","# resulted in around 1000 images, so we'll add time to the date range\n","# 10 years from the start of 2011 to the end of 2020\n","start_ = datetime(2011, 1, 1)\n","end_ = datetime(2020, 12, 31)\n","# Run the scraping pipeline using custom RedditScrape class imported above\n","cat = RedditScrape('cat', start_, end_)\n","cat.scrape_posts()\n","cat.posts.to_csv('data/tbl/cat_posts.csv', index=False)\n","dog = RedditScrape('dog', start_, end_)\n","dog.scrape_posts()\n","dog.posts.to_csv('data/tbl/dog_posts.csv', index=False)"]},{"cell_type":"markdown","source":["## Using `PRAW`\n","Now use `praw` to fetch additional information about each post now that it has been efficiently scraped using `psaw`. This will help us gather more information that will help more accurately filter for good posts/images, and acquire information that might be useful in modeling `score`."],"metadata":{"id":"KBG8O9zQjU4I"}},{"cell_type":"code","source":["reddit = praw.Reddit(\n","    client_id = 'client id',\n","    client_secret = 'client secret',\n","    username = 'username',\n","    password = 'password',\n","    user_agent = 'my user agent'\n",")\n","# Fill in all fields above\n","# more info: https://towardsdatascience.com/how-to-collect-a-reddit-dataset-c369de539114\n","# Now simply lookup the scraped posts on reddit's praw using the id obtained by psaw\n","# Then use the following method to get more data for each post:\n","# post = reddit.submission(id = ID)\n","# where ID is the value in column \"id\" in the data obtained from psaw"],"metadata":{"id":"2oj2AkdPjnlH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}