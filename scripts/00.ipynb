{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19193,"status":"ok","timestamp":1671982981409,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"},"user_tz":360},"id":"e3HytLQt-3G1","outputId":"6d8e0512-02e9-4b94-af1d-ee2b5895414a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"UD04Uk8pF5U9","executionInfo":{"status":"ok","timestamp":1671983032388,"user_tz":360,"elapsed":117,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jKF-p4HH_Cbo","executionInfo":{"status":"ok","timestamp":1671983035317,"user_tz":360,"elapsed":932,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Projects/reddit-vote-predictor')\n","import sys\n","import pandas as pd\n","from datetime import datetime\n","# Import custom module\n","from scripts.RedditScrape import * # imports a class `RedditScrape`"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6989,"status":"ok","timestamp":1671983062242,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"},"user_tz":360},"id":"k-p-tfQaGg8r","outputId":"52972b89-ca83-48c8-eac6-85bd7f444ca3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting psaw\n","  Downloading psaw-0.1.0-py3-none-any.whl (15 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from psaw) (2.23.0)\n","Requirement already satisfied: Click in /usr/local/lib/python3.8/dist-packages (from psaw) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->psaw) (2022.12.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->psaw) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->psaw) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->psaw) (2.10)\n","Installing collected packages: psaw\n","Successfully installed psaw-0.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting praw\n","  Downloading praw-7.6.1-py3-none-any.whl (188 kB)\n","\u001b[K     |████████████████████████████████| 188 kB 38.0 MB/s \n","\u001b[?25hCollecting websocket-client>=0.54.0\n","  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n","\u001b[?25hCollecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Collecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n","Installing collected packages: websocket-client, update-checker, prawcore, praw\n","Successfully installed praw-7.6.1 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.4.2\n"]}],"source":["# Reddit's own api:\n","!pip3 install psaw\n","!pip3 install praw\n","import psaw\n","import praw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1JiHU41_VP_"},"outputs":[],"source":["# Set date range from which to extract data for two subreddits\n","# NOTE: ater running the data scraping and cleaning pipeline, one year of data\n","# resulted in around 1000 images, so we'll add time to the date range\n","# 10 years from the start of 2011 to the end of 2020\n","start_ = datetime(2011, 1, 1)\n","end_ = datetime(2020, 12, 31)\n","# Run the scraping pipeline using custom RedditScrape class imported above\n","cat = RedditScrape('cat', start_, end_)\n","cat.scrape_posts()\n","cat.posts.to_csv('data/tbl/cat_posts.csv', index=False)\n","dog = RedditScrape('dog', start_, end_)\n","dog.scrape_posts()\n","dog.posts.to_csv('data/tbl/dog_posts.csv', index=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1306,"status":"ok","timestamp":1671983409858,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"},"user_tz":360},"id":"822ygeALRX7k"},"outputs":[],"source":["# If the above is not run, read in the data\n","cat = pd.read_csv('data/tbl/cat_posts.csv', index_col = None)\n","dog = pd.read_csv('data/tbl/dog_posts.csv', index_col = None)"]},{"cell_type":"markdown","metadata":{"id":"KBG8O9zQjU4I"},"source":["## Using `PRAW`\n","Now use `praw` to fetch additional information about each post now that it has been efficiently scraped using `psaw`. This will help us gather more information that will help more accurately filter for good posts/images, and acquire information that might be useful in modeling `score`.\n","\n","Use the `id` field of the `psaw` results to get more information for each post."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oj2AkdPjnlH"},"outputs":[],"source":["# reddit = praw.Reddit(\n","#     client_id = 'client id',\n","#     client_secret = 'client secret',\n","#     username = 'username',\n","#     password = 'password',\n","#     user_agent = 'my user agent'\n","# )\n","# Fill in all fields above\n","# Docs: https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\n","# Now simply lookup the scraped posts on reddit's praw using the id obtained by psaw\n","# Then use the following method to get more data for each post:\n","# post = reddit.submission(id = ID)\n","# where ID is the value in column \"id\" in the data obtained from psaw"]},{"cell_type":"markdown","metadata":{"id":"2PT8nt_qhEM6"},"source":["The above arguments are saved in a secret json file."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"cOiDZiVQdUnw","executionInfo":{"status":"ok","timestamp":1671983062480,"user_tz":360,"elapsed":241,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"}}},"outputs":[],"source":["import json\n","with open('reddit_args.json') as f:\n","  args = json.load(f)"]},{"cell_type":"markdown","metadata":{"id":"39yRB6MX_m54"},"source":["### Now use the args (which is a dict of keyword arguments) to set up the instance of the reddit praw api Reddit class"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oy3yVRbm_oA2","executionInfo":{"status":"ok","timestamp":1671983065596,"user_tz":360,"elapsed":133,"user":{"displayName":"Ian Douglas","userId":"15027398629126202839"}}},"outputs":[],"source":["reddit = praw.Reddit(**args, check_for_async=False)"]},{"cell_type":"markdown","metadata":{"id":"DJLqaqZgrIiy"},"source":["Using each post in the data scraped using `psaw`, we search for the exact submission using the `reddit` object created above. Each submission has attributes:\n","\n","* `submission.score` # the score upvotes - downvotes\n","* `submission.ups` # the number of upvotes\n","* `submission.downs` # the number of downvotes\n","\n","Note that `score` also downloaded with he `psaw` scrape but is not as accurate as the one downloaded from `praw` and we'll use that instead.\n"]},{"cell_type":"code","source":["# Set up a function to get the desired data if the submission is still up.\n","# If it no longer exists then it would throw a NotFound error, so use the\n","# class method fetch_data() to try to pull the data initially, and if it exists\n","# actually obtain the 3 columns desired\n","def try_get_data(submission):\n","  try:\n","    submission._fetch_data()\n","  except:\n","    out = [None]*3\n","  else:\n","    out = [getattr(submission, attr) for attr in ['score', 'ups', 'downs']]\n","  return out"],"metadata":{"id":"nz3q-dBkAfUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"63ATE2vj9rMw"},"outputs":[],"source":["# Set up lists to store outputs\n","cat_praw = []\n","dog_praw = []\n","# Iterate through submissions in cat and dog data\n","for i in range(cat.shape[0]):\n","  id_ = cat.id[i] # get the submission's id\n","  submission = reddit.submission(id_) # use to PRAW to find this submission\n","  data_ = try_get_data(submission) # try to get score, upvotes, and downvotes if exists\n","  cat_praw.append(data_) # add to list of outputs\n","# convert output list to data frame\n","cat_scores = pd.DataFrame(cat_praw, columns = ['praw_score', 'upvotes', 'downvotes'])\n","\n","# Repeat for dog\n","for i in range(dog.shape[0]):\n","  id_ = dog.id[i]\n","  submission = reddit.submission(id_)\n","  data_ = try_get_data(submission)\n","  dog_praw.append(data_)\n","dog_scores = pd.DataFrame(dog_praw, columns = ['praw_score', 'upvotes', 'downvotes'])"]},{"cell_type":"markdown","metadata":{"id":"6qhXbMFWTFr5"},"source":["### Now combine the variables scraped from `praw` with data from `psaw`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j8I_zmsB_50g"},"outputs":[],"source":["cat_new = pd.concat([cat, cat_praw], axis = 1)\n","dog_new = pd.concat([dog, dog_praw], axis= 1)\n","cat_new.to_csv('data/tbl/dog_posts.csv', index=False)\n","dog_new.to_csv('data/tbl/dog_posts.csv', index=False)"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}